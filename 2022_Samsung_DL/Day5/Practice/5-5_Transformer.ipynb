{"cells":[{"cell_type":"markdown","metadata":{"id":"8cPNLFZDkUQx"},"source":["Reference: https://www.youtube.com/watch?v=M6adRGJe5cQ\u0026t=180s"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22276,"status":"ok","timestamp":1645168914091,"user":{"displayName":"Jahn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11746682764249911445"},"user_tz":-540},"id":"VAie6QlvkAbx","outputId":"4ba93b25-af9a-45a6-e8ce-718b850f372e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting en_core_web_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n","\u001b[K     |████████████████████████████████| 12.0 MB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: spacy\u003e=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: numpy\u003e=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.21.5)\n","Requirement already satisfied: blis\u003c0.5.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: srsly\u003c1.1.0,\u003e=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (4.62.3)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: plac\u003c1.2.0,\u003e=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: wasabi\u003c1.1.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (0.9.0)\n","Requirement already satisfied: catalogue\u003c1.1.0,\u003e=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: importlib-metadata\u003e=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (4.11.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=0.20-\u003ecatalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=0.20-\u003ecatalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (3.7.0)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (2021.10.8)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/en_core_web_sm --\u003e\n","/usr/local/lib/python3.7/dist-packages/spacy/data/en\n","You can now load the model via spacy.load('en')\n","Collecting de_core_news_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9 MB)\n","\u001b[K     |████████████████████████████████| 14.9 MB 5.5 MB/s \n","\u001b[?25hRequirement already satisfied: spacy\u003e=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: numpy\u003e=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (1.21.5)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: wasabi\u003c1.1.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (0.9.0)\n","Requirement already satisfied: plac\u003c1.2.0,\u003e=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (4.62.3)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: srsly\u003c1.1.0,\u003e=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: blis\u003c0.5.0,\u003e=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: catalogue\u003c1.1.0,\u003e=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: importlib-metadata\u003e=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (4.11.0)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=0.20-\u003ecatalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (3.7.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata\u003e=0.20-\u003ecatalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (2021.10.8)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003ede_core_news_sm==2.2.5) (3.0.4)\n","Building wheels for collected packages: de-core-news-sm\n","  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-py3-none-any.whl size=14907055 sha256=8e130630394f4d465da207c10045d2f5df440971ce7d9919ddb0157535fd826f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-_17t5ej2/wheels/00/66/69/cb6c921610087d2cab339062345098e30a5ceb665360e7b32a\n","Successfully built de-core-news-sm\n","Installing collected packages: de-core-news-sm\n","Successfully installed de-core-news-sm-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('de_core_news_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/de_core_news_sm --\u003e\n","/usr/local/lib/python3.7/dist-packages/spacy/data/de\n","You can now load the model via spacy.load('de')\n","Collecting torchtext==0.6.0\n","  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 740 kB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.62.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.10.0+cu111)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 12.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.21.5)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etorchtext==0.6.0) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etorchtext==0.6.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etorchtext==0.6.0) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etorchtext==0.6.0) (2021.10.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch-\u003etorchtext==0.6.0) (3.10.0.2)\n","Installing collected packages: sentencepiece, torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.11.0\n","    Uninstalling torchtext-0.11.0:\n","      Successfully uninstalled torchtext-0.11.0\n","Successfully installed sentencepiece-0.1.96 torchtext-0.6.0\n"]}],"source":["!python -m spacy download en\n","!python -m spacy download de\n","!pip install torchtext==0.6.0"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6443,"status":"ok","timestamp":1645168929099,"user":{"displayName":"Jahn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11746682764249911445"},"user_tz":-540},"id":"LSoqv_9RYR8J"},"outputs":[],"source":["import torch\n","import spacy\n","from torchtext.data.metrics import bleu_score\n","import sys\n","\n","\n","def translate_sentence(model, sentence, german, english, device, max_length=50):\n","    # Load german tokenizer\n","    spacy_ger = spacy.load(\"de\")\n","\n","    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n","    if type(sentence) == str:\n","        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n","    else:\n","        tokens = [token.lower() for token in sentence]\n","\n","    # Add \u003cSOS\u003e and \u003cEOS\u003e in beginning and end respectively\n","    tokens.insert(0, german.init_token)\n","    tokens.append(german.eos_token)\n","\n","    # Go through each german token and convert to an index\n","    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n","\n","    # Convert to Tensor\n","    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n","\n","    outputs = [english.vocab.stoi[\"\u003csos\u003e\"]]\n","    for i in range(max_length):\n","        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n","\n","        with torch.no_grad():\n","            output = model(sentence_tensor, trg_tensor)\n","\n","        best_guess = output.argmax(2)[-1, :].item()\n","        outputs.append(best_guess)\n","\n","        if best_guess == english.vocab.stoi[\"\u003ceos\u003e\"]:\n","            break\n","\n","    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n","    # remove start token\n","    return translated_sentence[1:]\n","\n","\n","def bleu(data, model, german, english, device):\n","    targets = []\n","    outputs = []\n","\n","    for example in data:\n","        src = vars(example)[\"src\"]\n","        trg = vars(example)[\"trg\"]\n","\n","        prediction = translate_sentence(model, src, german, english, device)\n","        prediction = prediction[:-1]  # remove \u003ceos\u003e token\n","\n","        targets.append([trg])\n","        outputs.append(prediction)\n","\n","    return bleu_score(outputs, targets)\n","\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=\u003e Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=\u003e Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ZvbfpbKxaVP5"},"outputs":[{"name":"stdout","output_type":"stream","text":["downloading training.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01\u003c00:00, 625kB/s] \n"]},{"name":"stdout","output_type":"stream","text":["downloading validation.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00\u003c00:00, 226kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["downloading mmt_task1_test2016.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00\u003c00:00, 218kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["[Epoch 0 / 10]\n","=\u003e Saving checkpoint\n","Translated example sentence: \n"," ['yarn', 'prayer', 'pours', 'prayer', 'underway', 'peek', 'banging', 'pours', 'prayer', 'littered', 'been', 'coaster', 'venue', 'been', 'coaster', 'prayer', 'few', 'pours', 'prayer', 'retrieving', 'headgear', 'underway', 'littered', 'been', 'coaster', 'underway', 'peek', 'cardigan', 'been', 'yarn', 'underway', 'site', 'pastor', 'been', 'coaster', 'it', 'yarn', 'guides', 'yarn', 'underway', 'hooked', 'prayer', 'few', 'operated', 'yarn', 'underway', 'surfs', 'few', 'been', 'prayer']\n","[Epoch 1 / 10]\n","=\u003e Saving checkpoint\n","Translated example sentence: \n"," ['a', 'horse', 'walks', 'under', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '\u003ceos\u003e']\n","[Epoch 2 / 10]\n","=\u003e Saving checkpoint\n","Translated example sentence: \n"," ['a', 'horse', 'is', 'walking', 'under', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '\u003ceos\u003e']\n","[Epoch 3 / 10]\n","=\u003e Saving checkpoint\n","Translated example sentence: \n"," ['a', 'horse', 'is', 'walking', 'under', 'a', 'boat', 'next', 'to', 'a', 'boat', '.', '\u003ceos\u003e']\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import spacy\n","from torch.utils.tensorboard import SummaryWriter\n","from torchtext.datasets import Multi30k\n","from torchtext.data import Field, BucketIterator\n","\n","spacy_ger = spacy.load(\"de\")\n","spacy_eng = spacy.load(\"en\")\n","\n","def tokenize_ger(text):\n","    return [tok.text for tok in spacy_ger.tokenizer(text)]\n","\n","def tokenize_eng(text):\n","    return [tok.text for tok in spacy_eng.tokenizer(text)]\n","\n","\n","german = Field(tokenize=tokenize_ger, lower=True, init_token=\"\u003csos\u003e\", eos_token=\"\u003ceos\u003e\")\n","\n","english = Field(\n","    tokenize=tokenize_eng, lower=True, init_token=\"\u003csos\u003e\", eos_token=\"\u003ceos\u003e\"\n",")\n","\n","train_data, valid_data, test_data = Multi30k.splits(\n","    exts=(\".de\", \".en\"), fields=(german, english)\n",")\n","\n","german.build_vocab(train_data, max_size=10000, min_freq=2)\n","english.build_vocab(train_data, max_size=10000, min_freq=2)\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        embedding_size,\n","        src_vocab_size,\n","        trg_vocab_size,\n","        src_pad_idx,\n","        num_heads,\n","        num_encoder_layers,\n","        num_decoder_layers,\n","        forward_expansion,\n","        dropout,\n","        max_len,\n","        device,\n","    ):\n","        super(Transformer, self).__init__()\n","        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n","        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n","        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n","        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n","\n","        self.device = device\n","        self.transformer = nn.Transformer(\n","            embedding_size,\n","            num_heads,\n","            num_encoder_layers,\n","            num_decoder_layers,\n","            forward_expansion,\n","            dropout,\n","        )\n","        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","        self.src_pad_idx = src_pad_idx\n","\n","    def make_src_mask(self, src):\n","        src_mask = src.transpose(0, 1) == self.src_pad_idx\n","\n","        # (N, src_len)\n","        return src_mask.to(self.device)\n","\n","    def forward(self, src, trg):\n","        src_seq_length, N = src.shape\n","        trg_seq_length, N = trg.shape\n","\n","        src_positions = (\n","            torch.arange(0, src_seq_length)\n","            .unsqueeze(1)\n","            .expand(src_seq_length, N)\n","            .to(self.device)\n","        )\n","\n","        trg_positions = (\n","            torch.arange(0, trg_seq_length)\n","            .unsqueeze(1)\n","            .expand(trg_seq_length, N)\n","            .to(self.device)\n","        )\n","\n","        embed_src = self.dropout(\n","            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n","        )\n","        embed_trg = self.dropout(\n","            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n","        )\n","\n","        src_padding_mask = self.make_src_mask(src)\n","        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n","            self.device\n","        )\n","\n","        out = self.transformer(\n","            embed_src,\n","            embed_trg,\n","            src_key_padding_mask=src_padding_mask,\n","            tgt_mask=trg_mask,\n","        )\n","        out = self.fc_out(out)\n","        return out\n","\n","\n","# We're ready to define everything we need for training our Seq2Seq model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","load_model = True\n","save_model = True\n","\n","# Training hyperparameters\n","num_epochs = 10\n","learning_rate = 3e-4\n","batch_size = 32\n","\n","# Model hyperparameters\n","src_vocab_size = len(german.vocab)\n","trg_vocab_size = len(english.vocab)\n","embedding_size = 512\n","num_heads = 8\n","num_encoder_layers = 3\n","num_decoder_layers = 3\n","dropout = 0.10\n","max_len = 100\n","forward_expansion = 4\n","src_pad_idx = english.vocab.stoi[\"\u003cpad\u003e\"]\n","\n","# Tensorboard to get nice loss plot\n","writer = SummaryWriter(\"runs/loss_plot\")\n","step = 0\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size=batch_size,\n","    sort_within_batch=True,\n","    sort_key=lambda x: len(x.src),\n","    device=device,\n",")\n","\n","model = Transformer(\n","    embedding_size,\n","    src_vocab_size,\n","    trg_vocab_size,\n","    src_pad_idx,\n","    num_heads,\n","    num_encoder_layers,\n","    num_decoder_layers,\n","    forward_expansion,\n","    dropout,\n","    max_len,\n","    device,\n",").to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, factor=0.1, patience=10, verbose=True\n",")\n","\n","pad_idx = english.vocab.stoi[\"\u003cpad\u003e\"]\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n","\n","# if load_model:\n","#     load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n","\n","sentence = \"ein pferd geht unter einer brücke neben einem boot.\" # -\u003e english: A horse walks under a bridge next to a boat.\n","\n","for epoch in range(num_epochs):\n","    print(f\"[Epoch {epoch} / {num_epochs}]\")\n","\n","    if save_model:\n","        checkpoint = {\n","            \"state_dict\": model.state_dict(),\n","            \"optimizer\": optimizer.state_dict(),\n","        }\n","        save_checkpoint(checkpoint)\n","\n","    model.eval()\n","    translated_sentence = translate_sentence(\n","        model, sentence, german, english, device, max_length=50\n","    )\n","\n","    print(f\"Translated example sentence: \\n {translated_sentence}\")\n","    model.train()\n","    losses = []\n","\n","    for batch_idx, batch in enumerate(train_iterator):\n","        # Get input and targets and get to cuda\n","        inp_data = batch.src.to(device)\n","        target = batch.trg.to(device)\n","\n","        # Forward prop\n","        output = model(inp_data, target[:-1, :])\n","\n","        output = output.reshape(-1, output.shape[2])\n","        target = target[1:].reshape(-1)\n","\n","        optimizer.zero_grad()\n","\n","        loss = criterion(output, target)\n","        losses.append(loss.item())\n","\n","        # Back prop\n","        loss.backward()\n","\n","        # Clip to avoid exploding gradient issues, makes sure grads are\n","        # within a healthy range\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","        # Gradient descent step\n","        optimizer.step()\n","\n","        # plot to tensorboard\n","        writer.add_scalar(\"Training loss\", loss, global_step=step)\n","        step += 1\n","\n","    mean_loss = sum(losses) / len(losses)\n","    scheduler.step(mean_loss)\n","\n","# running on entire test data takes a while\n","score = bleu(test_data[1:100], model, german, english, device)\n","print(f\"Bleu score {score * 100:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHgIIY-hd42a"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir runs/loss_plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWzwVu2YB94b"},"outputs":[],"source":["sentence = \"Ich gehe zur Schule.\" # -\u003e english: I go to school.\n","translated_sentence = translate_sentence(model, sentence, german, english, device, max_length=50)\n","print(translated_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qy7PakFrtZkO"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"5-5_Transformer.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}